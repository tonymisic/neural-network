- Output layer should be a softmax activation function (not really beneficial for an XOR learning model)
- Visualization of Weights, biases and values to affirm correction of implementation so far
- Back-Propagation implementation
- Training data can be handled and ran on several epochs
- Testing implementation with a return of the error rate
- Compartmentalize into object/class that can be used easily
- Find suitable scalability test and show model can be learned

def back_propagate(Y):
    loss_der = error_derivative(Y[0], activations[-1][0])
    # calculate gradients
    for i in reversed(range(len(weights))):
        if i == len(weights) - 1: # output layer
            for j in range(len(biases[i])):
                L = len(biases[i])
                gradient_biases[i][j] = sigmoid_derivative(dot(weights[i][j*L:j*L+L], activations[i]) + biases[i][j]) * loss_der
            for j in range(len(weights[i])):
                L = len(biases[i])
                gradient_weights[i][j] = activations[i][j] * sigmoid_derivative(dot(weights[i][j*L:j*L+L], activations[i]) + biases[i][j]) * loss_der
        else: # all other layers
            pass
    # update biases with learning rate
    for i in range(len(gradient_biases)):
        for j in range(len(gradient_biases[i])):
            biases[i][j] += learning_rate * gradient_biases[i][j]
    # update weights with learning rate
    for i in range(len(gradient_weights)):
        for j in range(len(gradient_weights[i])):
            weights[i][j] += learning_rate * weights[i][j]
    print(loss_der)